{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = pd.read_csv(\"./data/mnist_train.csv\", sep=\",\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.loc[:, 1:] /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_frame.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=data[0,1:].reshape((28,28))\n",
    "data[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Podział na zbiór treningowy i walidacyjny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "train_data=data[0:50000,:]\n",
    "val_data=data[50000:,:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage as sk\n",
    "from skimage import transform\n",
    "import random\n",
    "import scipy\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_data[0,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation (x):\n",
    "    image=x.reshape(28,28)\n",
    "    random_degree = random.uniform(-40, 40)\n",
    "    rotated=sk.transform.rotate(image, random_degree)\n",
    "    return rotated.reshape(1,784)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image=data[0,1:]\n",
    "rotated = rotation(image)\n",
    "image_rotated=rotated.reshape(28,28)\n",
    "plt.imshow(image_rotated)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blurriness_translation (x):\n",
    "    vector=np.array((random.uniform(-5,5), random.uniform(-5,5)))\n",
    "    blurrness_ratio=randint(0,5)\n",
    "    transformed=scipy.ndimage.shift(data[0,1:].reshape(28,28), vector,order=blurrness_ratio,mode='constant')\n",
    "    return transformed.reshape(1,784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = blurriness_translation(image)\n",
    "image_transformed=transformed.reshape(28,28)\n",
    "plt.imshow(image_transformed)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescale\n",
    "def rescale (x):\n",
    "    x=x.reshape(28,28)\n",
    "    scale=random.uniform(5,9)/10\n",
    "    rescaled1=sk.transform.rescale(x, scale, mode='constant', multichannel=False)\n",
    "    padding_shape=int(np.floor((1-scale)*28/2)+1)\n",
    "    rescaled_with_padding=np.pad(rescaled1,(padding_shape,padding_shape),mode='constant', constant_values=0)\n",
    "    transformed=rescaled_with_padding[:28,:28]\n",
    "    return transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaled= rescale(image)\n",
    "plt.imshow(rescaled)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ZCA whitening\n",
    "#https://stackoverflow.com/questions/31528800/how-to-implement-zca-whitening-python\n",
    "\n",
    "\n",
    "def zca_whitening(x):\n",
    "    x=x.reshape(1,784)\n",
    "    sigma = np.dot(x, x.T)/x.shape[1] #Correlation matrix\n",
    "    U,S,V = np.linalg.svd(sigma) #Singular Value Decomposition\n",
    "    epsilon = 0.1                #Whitening constant, it prevents division by zero\n",
    "    ZCAMatrix = np.dot(np.dot(U, np.diag(1.0/np.sqrt(np.diag(S) + epsilon))), U.T)              #ZCA Whitening matrix\n",
    "    return np.dot(ZCAMatrix, x)   #Data whitening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitened= zca_whitening(image)\n",
    "plt.imshow(whitened.reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_noise (x):\n",
    "    noise = np.random.normal(0.0, 0.05, 784) \n",
    "    return x+noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy= random_noise(image)\n",
    "plt.imshow(noisy.reshape(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#applying modifications to some images\n",
    "image_transformations = {\n",
    "    0: rotation,\n",
    "    1: random_noise,\n",
    "    2: blurriness_translation,\n",
    "    3: zca_whitening,\n",
    "    4: rescale   \n",
    "}\n",
    "\n",
    "def apply_image_augmentation(data, n):\n",
    "    for i in range(n):\n",
    "        # random num of transformations to apply\n",
    "        to_apply=np.random.randint(0,2,len(image_transformations))\n",
    "        input_image=data[i,1:]\n",
    "        for i in range (len(image_transformations)):\n",
    "            if to_apply[i]==1:\n",
    "                input_image=image_transformations[i](input_image) \n",
    "                input_image=input_image.reshape(1,784)\n",
    "        modified=np.insert(input_image, -1,np.array(train_data[i,0]))\n",
    "        modified=modified.reshape(1,785)\n",
    "        data=np.append(data, modified,axis=0)   \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcje aktywacji wraz z ich pochodnymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_function(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def softmax_function(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def softmax_derivative(probabilities, layer_inputs):\n",
    "    number_of_classes = probabilities.shape[0]\n",
    "    diff_output = np.zeros([number_of_classes, number_of_classes])\n",
    "    for row in range(diff_output.shape[0]):\n",
    "        for col in range(diff_output.shape[1]):\n",
    "            if (col == row):\n",
    "                diff_output[row, col] = probabilities[row, 0] * (1 - probabilities[col, 0])\n",
    "            else:\n",
    "                diff_output[row, col] = -probabilities[row, 0] * probabilities[col, 0]\n",
    "    return np.matmul(diff_output, layer_inputs)\n",
    "\n",
    "def sigmoid_derivative(u):\n",
    "    sigmoid_value = sigmoid_function(u)\n",
    "    diff_output = sigmoid_value * (1 - sigmoid_value)\n",
    "    return diff_output\n",
    "\n",
    "def tanh_derivative(u):\n",
    "    tanh_value = tanh_function(u)\n",
    "    diff_output = 1 - np.power(tanh_value, 2)\n",
    "    return diff_output\n",
    "\n",
    "def tanh_function(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu_function(x):\n",
    "    return np.maximum(x,0)\n",
    "\n",
    "def relu_derivative(u):\n",
    "    relu_value=relu_function(u)\n",
    "    return (relu_value > 0) * 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funkcje kosztu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(prediction, target, eps = 1e-12):\n",
    "    prediction = prediction.T\n",
    "    target = target.T\n",
    "    ce = -np.sum(target*np.log(prediction + eps))\n",
    "    return ce\n",
    "\n",
    "def mse(predicted, true_label):\n",
    "    return np.mean(np.square(predicted - true_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(np.r_[0,0.8,0.1,0.1].reshape(4,1).T, np.r_[0,1,0,0].reshape(4,1).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(np.r_[0,0.3,0.3,0.4].reshape(4,1), np.r_[0,1,0,0].reshape(4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, log_loss\n",
    "mean_squared_error(np.r_[0,0.3,0.3,0.4].reshape(4,1), np.r_[0,1,0,0].reshape(4,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementacja sieci neuronowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "class Network(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_activation_functions_with_deriv():\n",
    "        return {\n",
    "            \"sigmoid\": [sigmoid_function, sigmoid_derivative],\n",
    "            \"tanh\": [tanh_function, tanh_derivative], \n",
    "            \"relu\": [relu_function, relu_derivative]\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_error_functions():\n",
    "        return {\n",
    "            \"mse\": mse,\n",
    "            \"cross_entropy\": cross_entropy\n",
    "        }\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_cost_functions():\n",
    "        return {\n",
    "            \"mse\": lambda outputs, inputs, labels, deriv: (outputs - labels) * \\\n",
    "                                                           deriv(outputs, inputs),\n",
    "            \"cross_entropy\": lambda outputs, inputs, labels, deriv: outputs - labels\n",
    "        }\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_neurons_num,\n",
    "                 hidden_layer_neurons_num,\n",
    "                 hidden_activation_functions_names,\n",
    "                 learning_rate=0.04,weight_decay=0.005, momentum=0.9,\n",
    "                 error_function_name=\"cross_entropy\", regularization=True, momentum_presence=True):\n",
    "        self.input_neurons_num = input_neurons_num\n",
    "        self.number_of_classes = 10\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay=weight_decay\n",
    "        self.momentum=momentum\n",
    "        self.regularization=regularization\n",
    "        self.momentum_presence=momentum_presence\n",
    "        self.neurons_num_per_layer = [self.input_neurons_num] + hidden_layer_neurons_num + [self.number_of_classes]\n",
    "        self.layers_num = len(self.neurons_num_per_layer)\n",
    "        self.activation_functions = self.__get_activation_functions(hidden_activation_functions_names)\n",
    "        self.activation_deriv_functions = self.__get_deriv_activation_functions(hidden_activation_functions_names)\n",
    "        self.error_function = Network.get_error_functions()[error_function_name]\n",
    "        self.cost_function = Network.get_cost_functions()[error_function_name]\n",
    "        self.weight_list = Network.__get_weight_list(self.neurons_num_per_layer)\n",
    "        self.bias_list = Network.__get_bias_list(self.neurons_num_per_layer)\n",
    "\n",
    "    def __get_deriv_activation_functions(self, inner_activation_functions_names):\n",
    "        functions = Network.__transform_to_activation_func_with_deriv(inner_activation_functions_names)\n",
    "        return list(map(lambda x: x[1], functions)) + \\\n",
    "            [lambda prob, layer_inputs: softmax_derivative(probabilities=prob, layer_inputs=layer_inputs)]\n",
    "    \n",
    "    def __get_activation_functions(self, inner_activation_functions_names):\n",
    "        functions = Network.__transform_to_activation_func_with_deriv(inner_activation_functions_names)\n",
    "        return list(map(lambda x: x[0], functions)) + \\\n",
    "            [lambda x: softmax_function(x)]\n",
    "    \n",
    "    @staticmethod\n",
    "    def __transform_to_activation_func_with_deriv(inner_activation_functions_names):\n",
    "        functions = Network.get_activation_functions_with_deriv()\n",
    "        return map(lambda name: functions[name], inner_activation_functions_names)\n",
    "        \n",
    "    @staticmethod\n",
    "    def __get_bias_list(neurons_num_per_layer):\n",
    "        return [np.random.randn(layer_num, 1) for layer_num in neurons_num_per_layer[1:]]\n",
    "\n",
    "    @staticmethod\n",
    "    def __get_weight_list(neurons_num_per_layer):\n",
    "        # Xavier initialization\n",
    "        return [np.random.randn(next_layer_num, current_layer_num) *\n",
    "                np.sqrt(6 / (next_layer_num + current_layer_num))\n",
    "                for (current_layer_num, next_layer_num) in zip(neurons_num_per_layer, neurons_num_per_layer[1:])]\n",
    "\n",
    "    def train(self, train_data, val_data, train_batch_size, epochs):\n",
    "        train_avg_per_epoch = np.zeros(epochs)\n",
    "        val_avg_per_epoch = np.zeros(epochs)\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(train_data)\n",
    "            train_batches = Network.__create_batches(train_data, train_batch_size)\n",
    "            train_cost = self.__train_one_epoch(train_batches)\n",
    "            train_avg_per_epoch[epoch] = train_cost/ train_data.shape[0]\n",
    "            _, val_cost= self.predict(val_data, False)\n",
    "            val_avg_per_epoch[epoch] = val_cost / val_data.shape[0]\n",
    "        return train_avg_per_epoch, val_avg_per_epoch\n",
    "\n",
    "    def predict(self, test_data, with_stat):\n",
    "        labels = Network.one_hot_encoded(test_data[:, 0], self.number_of_classes)\n",
    "        features = test_data[:, 1:]\n",
    "        res = []\n",
    "        diff = 0\n",
    "        stat = np.zeros((test_data.shape[0], 2)) if with_stat else None\n",
    "        for i in range(len(test_data)):\n",
    "            _, layer_output_list = self.__feed_forward(features[i, :])\n",
    "            output = layer_output_list[-1]\n",
    "            if with_stat:\n",
    "                stat[i, 0] = np.argmax(output)\n",
    "                stat[i, 1] = np.argmax(labels[:, i])    \n",
    "            diff += self.error_function(output, labels[:, i])\n",
    "        return stat, diff\n",
    "    \n",
    "    def __train_one_epoch(self, train_batches):\n",
    "        full_cost = 0\n",
    "        grad_w_prev = [0] * (self.layers_num - 1)\n",
    "        for train_batch in train_batches:\n",
    "            labels = Network.one_hot_encoded(train_batch[:, 0], self.number_of_classes)\n",
    "            train_batch_features = train_batch[:, 1:]\n",
    "            batch_size = train_batch_features.shape[0]\n",
    "            grad_b, grad_w, cost = self.__get_gradients(labels, train_batch, train_batch_features)\n",
    "            full_cost += cost\n",
    "            self.__update_network_params(batch_size, grad_b, grad_w, grad_w_prev)\n",
    "        return full_cost\n",
    "\n",
    "    def __update_network_params(self, batch_size, grad_b, grad_w, grad_w_prev):\n",
    "        if (self.momentum_presence==True):\n",
    "                momentum_term_w = [ self.momentum*w_prev/self.learning_rate for w_prev in grad_w_prev]\n",
    "                grad_w=[a+b for a, b in zip(grad_w,momentum_term_w)]\n",
    "                \n",
    "        self.weight_list = [w - (self.learning_rate * gw / batch_size)\n",
    "                            for w, gw in zip(self.weight_list, grad_w)]\n",
    "        self.bias_list = [b - (self.learning_rate * gb / batch_size)\n",
    "                          for b, gb in zip(self.bias_list, grad_b)]\n",
    "\n",
    "    def __get_gradients(self, labels, train_batch, train_batch_features):\n",
    "        cost = 0\n",
    "        grad_w = [0] * (self.layers_num - 1)\n",
    "        grad_b = [0] * (self.layers_num - 1)\n",
    "        for i in range(len(train_batch)):  # rekord\n",
    "            layer_inputs, layer_outputs = self.__feed_forward(train_batch_features[i, :])\n",
    "            cost += self.error_function(layer_outputs[-1], labels[:, i].reshape(self.number_of_classes, -1))\n",
    "           \n",
    "            # backprop\n",
    "            delta = self.__get_deltas(labels[:, i], layer_inputs, layer_outputs)\n",
    "            one_grad_w = [np.matmul(delta[0], train_batch_features[i, :].reshape(self.input_neurons_num, -1).T)]\n",
    "            for activ, d, u in zip(self.activation_functions, delta[1:], layer_inputs):\n",
    "                one_grad_w.append(np.matmul(d, activ(u).T))\n",
    "\n",
    "            grad_w = [a + b for a, b in zip(grad_w, one_grad_w)]\n",
    "            grad_b = [a + b for a, b in zip(delta, grad_b)]\n",
    "            \n",
    "            if (self.regularization==True):\n",
    "                regularization_term = [ self.weight_decay*w for w in self.weight_list]\n",
    "                grad_w=[a+b for a, b in zip(grad_w,regularization_term)]\n",
    "        return grad_b, grad_w, cost\n",
    "\n",
    "    def __get_deltas(self, labels, layer_inputs, layer_outputs):\n",
    "        labels = labels.reshape(self.number_of_classes, -1)\n",
    "        delta = [self.cost_function(layer_outputs[-1],\n",
    "                                    layer_inputs[-1],\n",
    "                                    labels, \n",
    "                                    self.activation_deriv_functions[-1])]\n",
    "        for layer_num in reversed(range(1, self.layers_num - 1)):\n",
    "            delta.append(np.matmul(self.weight_list[layer_num].T, delta[-1]) *\n",
    "                         self.activation_deriv_functions[layer_num - 1](layer_inputs[layer_num - 1]))\n",
    "        delta = list(reversed(delta))\n",
    "        return delta\n",
    "\n",
    "    def __feed_forward(self, features):\n",
    "        layer_inputs = []\n",
    "        layer_output = [features]\n",
    "        for activation_function, w, b in zip(self.activation_functions,\n",
    "                                             self.weight_list,\n",
    "                                             self.bias_list):\n",
    "            a = np.matmul(w, layer_output[-1])\n",
    "            layer_inputs.append(a.reshape(a.shape[0], 1) + b)\n",
    "            layer_output.append(activation_function(layer_inputs[-1]))\n",
    "        return layer_inputs, layer_output\n",
    "\n",
    "    @staticmethod\n",
    "    def __create_batches(data, batch_size):\n",
    "        rows_num = data.shape[0]\n",
    "        n = int(np.ceil(rows_num / batch_size))\n",
    "        return [data[(i * batch_size):min((i + 1) * batch_size, rows_num)] for i in range(n)]\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encoded(labels, number_of_classes):\n",
    "        labels = labels.astype(int)\n",
    "        res = np.zeros((number_of_classes, labels.shape[0]))\n",
    "        res[labels, np.arange(res.shape[1])] = 1\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testowanie implementacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych testowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"./data/mnist_test.csv\", sep=\",\", header=None)\n",
    "test_data.loc[:, 1:] /= 255.0\n",
    "test_data=test_data.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accurracy(stat):\n",
    "    count=0\n",
    "    for i in range (stat.shape[0]):\n",
    "        if stat[i,0]==stat[i,1]:\n",
    "            count=count+1\n",
    "    return count/stat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapisywanie wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_as_json(v, file_name):\n",
    "    output = []\n",
    "    for i in range(len(v)):\n",
    "        element = v[i]\n",
    "        element[\"df_wrong_predicted\"] = element[\"df_wrong_predicted\"].index.tolist()\n",
    "        element[\"stat\"] = element[\"stat\"].tolist()\n",
    "        element[\"train_cost\"] = element[\"train_cost\"].tolist()\n",
    "        element[\"val_cost\"] = element[\"val_cost\"].tolist()\n",
    "        output.append(element)\n",
    "    with open(file_name, \"w\") as f:\n",
    "        f.write(json.dumps(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wizualizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_plot(train_cost, val_cost):\n",
    "    plt.plot(train_cost, label = \"train\")\n",
    "    plt.plot(val_cost, label = \"val\")\n",
    "    plt.legend(loc = \"upper right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy nr 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=30,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(train_data, val_data, test_data, hidden_layer_neurons_num, hidden_activation_functions_names,\n",
    "         epochs,\n",
    "         image_augmentation,\n",
    "         train_batch_size=128,\n",
    "         learning_rate=0.04,\n",
    "         weight_decay=0.005,\n",
    "         momentum=0.9,\n",
    "         error_function_name=\"cross_entropy\",\n",
    "         regularization=False,\n",
    "         momentum_presence=False,\n",
    "         images_to_transform_num=1000):\n",
    "    \n",
    "    if image_augmentation:\n",
    "        apply_image_augmentation(train_data, images_to_transform_num)\n",
    "        \n",
    "    net = Network(input_neurons_num=784,\n",
    "                  hidden_layer_neurons_num=hidden_layer_neurons_num,\n",
    "                  hidden_activation_functions_names=hidden_activation_functions_names,\n",
    "                  learning_rate=learning_rate,\n",
    "                  weight_decay=weight_decay,\n",
    "                  momentum=momentum,\n",
    "                  error_function_name=error_function_name,\n",
    "                  regularization=regularization,\n",
    "                  momentum_presence=momentum_presence)\n",
    "    train_cost, val_cost = net.train(train_data, val_data, train_batch_size=train_batch_size, epochs=epochs)\n",
    "    stat, test_error = net.predict(test_data, with_stat=True)\n",
    "    df_stat = pd.DataFrame({\"predicted\": stat[:,0], \"wanted\": stat[:, 1]}).astype(int)\n",
    "    df_wrong_predicted = df_stat.loc[df_stat[\"predicted\"] != df_stat[\"wanted\"]]\n",
    "    accuracy = compute_accurracy(stat)\n",
    "    \n",
    "    print(\"accuracy {0}\".format(accuracy))\n",
    "    show_error_plot(train_cost, val_cost)\n",
    "    \n",
    "    return {\n",
    "        \"train_cost\": train_cost,\n",
    "        \"val_cost\": val_cost,\n",
    "        \"df_wrong_predicted\": df_wrong_predicted,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"hidden_layer_neurons_num\": hidden_layer_neurons_num,\n",
    "        \"hidden_activation_functions_names\": hidden_activation_functions_names,\n",
    "        \"epochs\": epochs,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"momentum\": momentum,\n",
    "        \"error_function_name\": error_function_name,\n",
    "        \"regularization\": regularization,\n",
    "        \"momentum_presence\": momentum_presence,\n",
    "        \"image_augmentation\": image_augmentation,\n",
    "        \"stat\": stat,\n",
    "        \"test_error\": test_error\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testy nr 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=30,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 4,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 4,\n",
    "    epochs=30,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[20] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=30,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[20] * 5,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 5,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[20] * 10,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 10,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[20] * 20,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 20,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[20] * 30,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 30,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(results, \"test_results_layer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testy nr 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[20] * 5,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 5,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True,\n",
    "    regularization=True,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=5,\n",
    "    error_function_name=\"mse\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=10,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"sigmoid\"] * 2,\n",
    "    epochs=5,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"sigmoid\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"sigmoid\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"relu\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"relu\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 1,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 1,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 1,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 1,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 3,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 3,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 3,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 3,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"mse\",\n",
    "    image_augmentation=False)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"mse\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True,\n",
    "    regularization=True,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 3,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 3,\n",
    "    epochs=20,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True,\n",
    "    regularization=True,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"tanh\"] * 2,\n",
    "    epochs=20,\n",
    "    error_function_name=\"mse\",\n",
    "    image_augmentation=False,\n",
    "    regularization=False,\n",
    "    momentum_presence=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "r = test(\n",
    "    train_data=train_data,\n",
    "    val_data=val_data,\n",
    "    test_data=test_data,\n",
    "    hidden_layer_neurons_num=[128] * 2,\n",
    "    hidden_activation_functions_names=[\"relu\"] * 2,\n",
    "    epochs=30,\n",
    "    error_function_name=\"cross_entropy\",\n",
    "    image_augmentation=True)\n",
    "results.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_as_json(results, \"test_results_more.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
